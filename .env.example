# Tarsis Configuration Example
# Copy this file to .env and fill in your values

# ==============================================================================
# GitHub Configuration (Required)
# ==============================================================================

# Your GitHub personal access token
# Create one at: https://github.com/settings/tokens
# Required scopes: repo, workflow
GITHUB_TOKEN=ghp_your_github_token_here

# Repository to monitor
GITHUB_REPO_OWNER=your-github-username
GITHUB_REPO_NAME=your-repository-name

# ==============================================================================
# LLM Provider Configuration (Choose One)
# ==============================================================================

# -----------------------------------------------------------------------------
# Option 1: Ollama (Local, Free) - RECOMMENDED FOR TESTING
# -----------------------------------------------------------------------------
LLM_PROVIDER=ollama
LLM_MODEL_ID=qwen2.5-coder:7b
OLLAMA_BASE_URL=http://localhost:11434

# Enable structured output (grammar-based tool calling)
# WARNING: Setting this to 'true' may cause llama.cpp grammar crashes with complex schemas
# Default: false (uses safer prompt-based tool calling)
# OLLAMA_STRUCTURED_OUTPUT=false

# Request timeout in seconds (default: 1800 = 30 minutes)
# For CPU-based inference, you may need to increase this
# Set to 0 for unlimited timeout (not recommended for production)
# OLLAMA_TIMEOUT=1800

# Other good Ollama models for coding:
# LLM_MODEL_ID=llama3.1:8b
# LLM_MODEL_ID=deepseek-coder:6.7b
# LLM_MODEL_ID=codellama:13b

# To use Ollama:
# 1. Install: curl -fsSL https://ollama.com/install.sh | sh
# 2. Pull model: ollama pull qwen2.5-coder:7b
# 3. Start server: ollama serve

# -----------------------------------------------------------------------------
# Option 2: Anthropic (Claude) - RECOMMENDED FOR PRODUCTION
# -----------------------------------------------------------------------------
# LLM_PROVIDER=anthropic
# LLM_MODEL_ID=claude-3-5-sonnet-20241022
# ANTHROPIC_API_KEY=sk-ant-your-api-key-here

# Available Claude models:
# claude-3-5-sonnet-20241022  (Best balance)
# claude-3-opus-20240229      (Most capable, slower)
# claude-3-haiku-20240307     (Fastest, cheapest)

# Get API key at: https://console.anthropic.com/

# -----------------------------------------------------------------------------
# Option 3: Google Gemini - EXPERIMENTAL (Tool Calling Limitations)
# -----------------------------------------------------------------------------
# LLM_PROVIDER=gemini
# LLM_MODEL_ID=gemini-2.5-flash
# GEMINI_API_KEY=your-api-key-here

# ⚠️ CRITICAL: Gemini has known reliability issues with complex tool calling:
#   - gemini-2.5-flash-lite: DO NOT USE - Completely broken (malformed calls, infinite loops)
#   - gemini-2.5-flash: CAUTION - Works but has occasional crashes (testing only)
#   - gemini-2.5-pro: BETTER - More reliable but still experimental

# Available Gemini models (Latest: 2.5):
# gemini-2.5-pro              (Most capable, reasoning over complex code/math, 1M context)
# gemini-2.5-flash            (Fast, efficient, high volume, 1M context - EXPERIMENTAL)
# gemini-2.5-flash-lite       (NOT RECOMMENDED - Tool calling issues)

# Recommended alternatives for production:
#   - Anthropic Claude (claude-3-5-sonnet-20241022) - Most reliable
#   - Ollama (qwen2.5-coder:7b) - Best local option

# Installation (new API):
# pip install google-genai

# Get API key at: https://aistudio.google.com/apikey

# -----------------------------------------------------------------------------
# Option 4: OpenAI (Coming Soon)
# -----------------------------------------------------------------------------
# LLM_PROVIDER=openai
# LLM_MODEL_ID=gpt-4-turbo
# OPENAI_API_KEY=sk-your-openai-key-here

# ==============================================================================
# Agent Configuration (Optional)
# ==============================================================================

# Maximum iterations per task (default: 25)
# MAX_ITERATIONS=25

# Maximum consecutive mistakes before stopping (default: 3)
# MAX_CONSECUTIVE_MISTAKES=3

# Auto-approve tool executions (default: false)
# AUTO_APPROVE=false

# ==============================================================================
# Server Configuration (Optional)
# ==============================================================================

# Host to bind to (default: 127.0.0.1)
# SERVER_HOST=127.0.0.1

# Port to bind to (default: 8000)
# SERVER_PORT=8000

# ==============================================================================
# Webhook Configuration (Optional)
# ==============================================================================

# Webhook secret for signature validation
# WEBHOOK_SECRET=your-webhook-secret-here

# ==============================================================================
# Logging Configuration (Optional)
# ==============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_LEVEL=INFO

# Log format: simple, detailed, json
# LOG_FORMAT=simple

# Include technical details (traceback) in GitHub error comments
# Default: false (shows only user-friendly messages)
# Set to true for debugging or development
# ERROR_INCLUDE_TRACEBACK=false

# ==============================================================================
# Retry Configuration (Optional)
# ==============================================================================

# Maximum number of retry attempts for failed API calls
# Applies to: LLM providers (Anthropic, Ollama, Gemini) and GitHub API
# Default: 3
# MAX_RETRIES=3

# Base delay in seconds for exponential backoff
# Each retry uses: base_delay * (exponential_base ^ attempt)
# Default: 1.0
# RETRY_BASE_DELAY=1.0

# Exponential base for backoff calculation
# Default: 2 (delays: 1s, 2s, 4s, 8s...)
# RETRY_EXPONENTIAL_BASE=2

# Maximum delay in seconds between retries
# Prevents exponential backoff from growing too large
# Default: 60.0
# RETRY_MAX_DELAY=60.0

# Add random jitter to retry delays (reduces thundering herd)
# Default: true
# RETRY_JITTER=true

# Note: Retries apply to network errors, rate limits (429), and server errors (5xx)
# Auth errors (401, 403) and validation errors (400, 422) are NOT retried

# ==============================================================================
# Advanced Configuration (Optional)
# ==============================================================================

# LLM temperature (0.0-1.0, default: 0.2)
# LLM_TEMPERATURE=0.2

# Max tokens per LLM request (default: 4096)
# LLM_MAX_TOKENS=4096

# Enable streaming (default: false)
# LLM_STREAMING=false

# ==============================================================================
# Local Repository Management
# ==============================================================================

# Workspace directory for local repository clones
# Default: /tmp/tarsis (temporary, cleaned automatically)
# Set to persistent path like ~/.tarsis/workspace for faster subsequent tasks
# TARSIS_WORKSPACE_DIR=/tmp/tarsis

# Cleanup local clones when task completes
# Default: true (recommended to prevent disk buildup)
# Set to false only for debugging or to inspect clone state
# TARSIS_CLEANUP_ON_EXIT=true

# ==============================================================================
# Advanced File Operations
# ==============================================================================

# File operation mode: how to handle file modifications
# - auto: Automatically choose best method (API for simple, local for complex)
# - api: Always use GitHub API (simpler, rate limited)
# - local: Always use local git (faster for bulk, requires clone)
# Default: auto (recommended)
# FILE_OPERATION_MODE=auto

# Enable symlink support
# Set to false to disable symlink creation tool
# Note: Symlinks require Developer Mode on Windows 10+ or admin privileges
# Default: true
# ENABLE_SYMLINKS=true

# Require platform check before allowing symlink creation
# If true, will check if system supports symlinks before allowing operations
# If false, will attempt symlink creation without checking (may fail on Windows)
# Default: true (recommended)
# SYMLINK_REQUIRE_CHECK=true

# ==============================================================================
# Commit Message Generation
# ==============================================================================

# Enable AI-powered commit message generation by default
# When true, all file modification tools will auto-generate commit messages
# unless explicitly provided by the user
# Default: false (user provides messages manually)
# COMMIT_AUTO_GENERATE_DEFAULT=false

# Validation strictness for generated commit messages
# - strict: Warnings treated as errors, message must be perfect
# - normal: Only errors block, warnings allowed
# - lenient: Minimal validation, focus on format only
# Default: normal
# COMMIT_VALIDATION_STRICTNESS=normal

# Maximum header length for commit messages (characters)
# Conventional commits recommend 50-72 characters for the header
# Default: 72
# COMMIT_MAX_HEADER_LENGTH=72

# Maximum line length for commit body (characters)
# Default: 100
# COMMIT_MAX_LINE_LENGTH=100

# Temperature for AI commit message generation (0.0-1.0)
# Lower = more focused/deterministic, Higher = more creative
# Default: 0.3 (recommended for commit messages)
# COMMIT_GENERATION_TEMPERATURE=0.3

# ==============================================================================
# Multi-Commit Support
# ==============================================================================

# Enable multi-commit mode by default for large changesets
# When true, tools will automatically use multi-commit mode for >5 files
# When false, single commit is default (opt-in via multi_commit parameter)
# Default: false (preserve backward compatibility)
# MULTI_COMMIT_DEFAULT=false

# Minimum files required to trigger multi-commit heuristic
# If changeset has fewer files than this, always use single commit
# Default: 5
# MULTI_COMMIT_MIN_FILES=5

# Maximum commits to create in multi-commit mode
# Prevents excessive fragmentation of large changesets
# Default: 5
# MULTI_COMMIT_MAX_COMMITS=5

# Minimum files per group (for TypeBasedGrouping strategy)
# Groups with fewer files may be merged with related groups
# Default: 2
# MULTI_COMMIT_MIN_GROUP_SIZE=2

# Merge threshold for small groups
# Groups with ≤ this many files may be merged together
# Default: 2
# MULTI_COMMIT_MERGE_THRESHOLD=2

# Maximum files per commit (for SizeBasedGrouping strategy)
# Commits exceeding this will be split into smaller groups
# Default: 15
# MULTI_COMMIT_MAX_FILES_PER_COMMIT=15

# Maximum lines of code per commit (for SizeBasedGrouping strategy)
# Commits exceeding this LOC will be split into smaller groups
# Default: 500
# MULTI_COMMIT_MAX_LOC_PER_COMMIT=500

# Grouping strategy priority order
# Comma-separated list: type,dependency,size
# Default: type,dependency,size
# MULTI_COMMIT_STRATEGY_ORDER=type,dependency,size

# Note: Multi-commit mode requires auto_generate_message=true
# Each commit group gets an AI-generated conventional commit message

# ==============================================================================
# Reflexion Framework - Self-Reflection and Learning from Mistakes
# ==============================================================================

# Enable Reflexion framework for self-reflection and learning
# When true, agent will reflect on failures and learn from mistakes
# Default: true (recommended for production)
# REFLEXION_ENABLED=true

# Reflexion execution mode
# - within_task: Reflect during single task execution (default, faster)
# - multi_trial: Trial-based learning with full restarts (more thorough)
# - hybrid: Within-task first, escalate to multi-trial if needed (best of both)
# Default: within_task
# REFLEXION_MODE=within_task

# Maximum number of reflection insights to keep in memory
# Older reflections are removed when limit is reached (FIFO)
# Higher = more context, but larger prompts
# Default: 10
# REFLEXION_MEMORY_SIZE=10

# Temperature for self-reflection LLM calls (0.0-1.0)
# Higher = more creative reflections, Lower = more focused
# Recommendation: 0.5 (higher than action generation temperature of 0.2)
# Default: 0.5
# REFLEXION_TEMPERATURE=0.5

# ==============================================================================
# Reflexion Triggers - When to Trigger Self-Reflection
# ==============================================================================

# Trigger reflection after validation failures (tests, linting, etc.)
# Most important trigger for code quality improvement
# Default: true (highly recommended)
# REFLEXION_TRIGGER_VALIDATION_FAILURE=true

# Trigger reflection after tool execution errors
# Helps learn codebase structure and avoid repeated tool failures
# Default: true (recommended)
# REFLEXION_TRIGGER_TOOL_ERROR=true

# Trigger reflection after N consecutive mistakes
# Set to 0 to disable, or number of mistakes to trigger reflection
# Prevents repeated errors and helps break failure patterns
# Default: 3
# REFLEXION_TRIGGER_CONSECUTIVE_MISTAKES=3

# Trigger periodic reflection every N iterations
# Set to 0 to disable, or number of iterations between reflections
# Useful for complex multi-step tasks to review progress
# Default: 5
# REFLEXION_TRIGGER_PERIODIC_INTERVAL=5

# Trigger pre-completion verification before marking task complete
# Checks all requirements are met before finishing the task
# CRITICAL for preventing incomplete implementations (highly recommended)
# Default: true
# REFLEXION_TRIGGER_PRE_COMPLETION=true

# ==============================================================================
# Multi-Trial Mode Settings
# ==============================================================================

# Maximum number of trials in multi-trial mode
# Agent will attempt task this many times, learning from each failure
# Only applies when REFLEXION_MODE=multi_trial or hybrid mode escalates
# Default: 5
# REFLEXION_MAX_TRIALS=5

# ==============================================================================
# Repository-Level Learning (Cross-Issue Memory)
# ==============================================================================

# Persist reflections across different GitHub issues
# When true, agent learns from past issues in the same repository
# When false, reflections are cleared after each issue completion
# Default: false (task-level learning only)
# REFLEXION_PERSIST_ACROSS_ISSUES=false

# Directory for storing repository-level reflection cache
# Reflections saved here are reused across issues for learning
# Only used when REFLEXION_PERSIST_ACROSS_ISSUES=true
# Default: .tarsis/reflections
# REFLEXION_REPO_CACHE_DIR=.tarsis/reflections

# Notes:
# - Repository cache files are stored as: {repo_owner}/{repo_name}/issue_{number}.json
# - Cache files older than 30 days are automatically cleaned up
# - Similarity matching used to find relevant past reflections for current task
