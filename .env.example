# Tarsis Configuration Example
# Copy this file to .env and fill in your values

# ==============================================================================
# GitHub Configuration (Required)
# ==============================================================================

# Your GitHub personal access token
# Create one at: https://github.com/settings/tokens
# Required scopes: repo, workflow
GITHUB_TOKEN=ghp_your_github_token_here

# Repository to monitor
GITHUB_REPO_OWNER=your-github-username
GITHUB_REPO_NAME=your-repository-name

# ==============================================================================
# LLM Provider Configuration (Choose One)
# ==============================================================================

# -----------------------------------------------------------------------------
# Option 1: Ollama (Local, Free) - RECOMMENDED FOR TESTING
# -----------------------------------------------------------------------------
LLM_PROVIDER=ollama
LLM_MODEL_ID=qwen2.5-coder:7b
OLLAMA_BASE_URL=http://localhost:11434

# Enable structured output (grammar-based tool calling)
# WARNING: Setting this to 'true' may cause llama.cpp grammar crashes with complex schemas
# Default: false (uses safer prompt-based tool calling)
# OLLAMA_STRUCTURED_OUTPUT=false

# Request timeout in seconds (default: 1800 = 30 minutes)
# For CPU-based inference, you may need to increase this
# Set to 0 for unlimited timeout (not recommended for production)
# OLLAMA_TIMEOUT=1800

# Other good Ollama models for coding:
# LLM_MODEL_ID=llama3.1:8b
# LLM_MODEL_ID=deepseek-coder:6.7b
# LLM_MODEL_ID=codellama:13b

# To use Ollama:
# 1. Install: curl -fsSL https://ollama.com/install.sh | sh
# 2. Pull model: ollama pull qwen2.5-coder:7b
# 3. Start server: ollama serve

# -----------------------------------------------------------------------------
# Option 2: Anthropic (Claude) - RECOMMENDED FOR PRODUCTION
# -----------------------------------------------------------------------------
# LLM_PROVIDER=anthropic
# LLM_MODEL_ID=claude-3-5-sonnet-20241022
# ANTHROPIC_API_KEY=sk-ant-your-api-key-here

# Available Claude models:
# claude-3-5-sonnet-20241022  (Best balance)
# claude-3-opus-20240229      (Most capable, slower)
# claude-3-haiku-20240307     (Fastest, cheapest)

# Get API key at: https://console.anthropic.com/

# -----------------------------------------------------------------------------
# Option 3: Google Gemini - FAST AND COST-EFFECTIVE
# -----------------------------------------------------------------------------
# LLM_PROVIDER=gemini
# LLM_MODEL_ID=gemini-2.5-flash
# GEMINI_API_KEY=your-api-key-here

# Available Gemini models (Latest: 2.5):
# gemini-2.5-pro              (Most capable, reasoning over complex code/math, 1M context)
# gemini-2.5-flash            (Recommended: Fast, efficient, high volume, 1M context)
# gemini-2.5-flash-lite       (Cost effective, low latency, 1M context)

# Get API key at: https://aistudio.google.com/apikey

# -----------------------------------------------------------------------------
# Option 4: OpenAI (Coming Soon)
# -----------------------------------------------------------------------------
# LLM_PROVIDER=openai
# LLM_MODEL_ID=gpt-4-turbo
# OPENAI_API_KEY=sk-your-openai-key-here

# ==============================================================================
# Agent Configuration (Optional)
# ==============================================================================

# Maximum iterations per task (default: 25)
# MAX_ITERATIONS=25

# Maximum consecutive mistakes before stopping (default: 3)
# MAX_CONSECUTIVE_MISTAKES=3

# Auto-approve tool executions (default: false)
# AUTO_APPROVE=false

# ==============================================================================
# Server Configuration (Optional)
# ==============================================================================

# Host to bind to (default: 127.0.0.1)
# SERVER_HOST=127.0.0.1

# Port to bind to (default: 8000)
# SERVER_PORT=8000

# ==============================================================================
# Webhook Configuration (Optional)
# ==============================================================================

# Webhook secret for signature validation
# WEBHOOK_SECRET=your-webhook-secret-here

# ==============================================================================
# Logging Configuration (Optional)
# ==============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_LEVEL=INFO

# Log format: simple, detailed, json
# LOG_FORMAT=simple

# Include technical details (traceback) in GitHub error comments
# Default: false (shows only user-friendly messages)
# Set to true for debugging or development
# ERROR_INCLUDE_TRACEBACK=false

# ==============================================================================
# Retry Configuration (Optional)
# ==============================================================================

# Maximum number of retry attempts for failed API calls
# Applies to: LLM providers (Anthropic, Ollama, Gemini) and GitHub API
# Default: 3
# MAX_RETRIES=3

# Base delay in seconds for exponential backoff
# Each retry uses: base_delay * (exponential_base ^ attempt)
# Default: 1.0
# RETRY_BASE_DELAY=1.0

# Exponential base for backoff calculation
# Default: 2 (delays: 1s, 2s, 4s, 8s...)
# RETRY_EXPONENTIAL_BASE=2

# Maximum delay in seconds between retries
# Prevents exponential backoff from growing too large
# Default: 60.0
# RETRY_MAX_DELAY=60.0

# Add random jitter to retry delays (reduces thundering herd)
# Default: true
# RETRY_JITTER=true

# Note: Retries apply to network errors, rate limits (429), and server errors (5xx)
# Auth errors (401, 403) and validation errors (400, 422) are NOT retried

# ==============================================================================
# Advanced Configuration (Optional)
# ==============================================================================

# LLM temperature (0.0-1.0, default: 0.2)
# LLM_TEMPERATURE=0.2

# Max tokens per LLM request (default: 4096)
# LLM_MAX_TOKENS=4096

# Enable streaming (default: false)
# LLM_STREAMING=false
